{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import random\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D, concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "import seaborn as sns\n",
    "from keras.datasets import mnist\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy.random as rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread(path):\n",
    "    im = cv2.imread(path)\n",
    "    im = cv2.resize(im, (105,105), interpolation = cv2.INTER_AREA)\n",
    "    a = np.asarray(im)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"Datasets/101_ObjectCategories/\"\n",
    "val_folder = 'Datasets/101_ObjectCategories/'\n",
    "save_path = 'Datasets/101_ObjectCategories/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadimg(path):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    curr_y=0\n",
    "    \n",
    "    for category in os.listdir(path):\n",
    "        print(\"loading category: \" + category)\n",
    "        categorypath = path + str(category)\n",
    "        i=0\n",
    "        for filename in os.listdir(categorypath):\n",
    "            image_path = categorypath + \"/\" + filename\n",
    "            image = imread(image_path)\n",
    "            X.append(image)\n",
    "            y.append(curr_y)\n",
    "            i=i+1\n",
    "            if i==30:\n",
    "                break\n",
    "        curr_y=curr_y+1\n",
    "    return X,y\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading category: accordion\n",
      "loading category: airplanes\n",
      "loading category: anchor\n",
      "loading category: ant\n",
      "loading category: BACKGROUND_Google\n",
      "loading category: barrel\n",
      "loading category: bass\n",
      "loading category: beaver\n",
      "loading category: binocular\n",
      "loading category: bonsai\n",
      "loading category: brain\n",
      "loading category: brontosaurus\n",
      "loading category: buddha\n",
      "loading category: butterfly\n",
      "loading category: camera\n",
      "loading category: cannon\n",
      "loading category: car_side\n",
      "loading category: ceiling_fan\n",
      "loading category: cellphone\n",
      "loading category: chair\n",
      "loading category: chandelier\n",
      "loading category: cougar_body\n",
      "loading category: cougar_face\n",
      "loading category: crab\n",
      "loading category: crayfish\n",
      "loading category: crocodile\n",
      "loading category: crocodile_head\n",
      "loading category: cup\n",
      "loading category: dalmatian\n",
      "loading category: dollar_bill\n",
      "loading category: dolphin\n",
      "loading category: dragonfly\n",
      "loading category: electric_guitar\n",
      "loading category: elephant\n",
      "loading category: emu\n",
      "loading category: euphonium\n",
      "loading category: ewer\n",
      "loading category: Faces\n",
      "loading category: Faces_easy\n",
      "loading category: ferry\n",
      "loading category: flamingo\n",
      "loading category: flamingo_head\n",
      "loading category: garfield\n",
      "loading category: gerenuk\n",
      "loading category: gramophone\n",
      "loading category: grand_piano\n",
      "loading category: hawksbill\n",
      "loading category: headphone\n",
      "loading category: hedgehog\n",
      "loading category: helicopter\n",
      "loading category: ibis\n",
      "loading category: inline_skate\n",
      "loading category: joshua_tree\n",
      "loading category: kangaroo\n",
      "loading category: ketch\n",
      "loading category: lamp\n",
      "loading category: laptop\n",
      "loading category: Leopards\n",
      "loading category: llama\n",
      "loading category: lobster\n",
      "loading category: lotus\n",
      "loading category: mandolin\n",
      "loading category: mayfly\n",
      "loading category: menorah\n",
      "loading category: metronome\n",
      "loading category: minaret\n",
      "loading category: Motorbikes\n",
      "loading category: nautilus\n",
      "loading category: octopus\n",
      "loading category: okapi\n",
      "loading category: pagoda\n",
      "loading category: panda\n",
      "loading category: pigeon\n",
      "loading category: pizza\n",
      "loading category: platypus\n",
      "loading category: pyramid\n",
      "loading category: revolver\n",
      "loading category: rhino\n",
      "loading category: rooster\n",
      "loading category: saxophone\n",
      "loading category: schooner\n",
      "loading category: scissors\n",
      "loading category: scorpion\n",
      "loading category: sea_horse\n",
      "loading category: snoopy\n",
      "loading category: soccer_ball\n",
      "loading category: stapler\n",
      "loading category: starfish\n",
      "loading category: stegosaurus\n",
      "loading category: stop_sign\n",
      "loading category: strawberry\n",
      "loading category: sunflower\n",
      "loading category: tick\n",
      "loading category: trilobite\n",
      "loading category: umbrella\n",
      "loading category: watch\n",
      "loading category: water_lilly\n",
      "loading category: wheelchair\n",
      "loading category: wild_cat\n",
      "loading category: windsor_chair\n",
      "loading category: wrench\n",
      "loading category: yin_yang\n"
     ]
    }
   ],
   "source": [
    "X,y=loadimg(train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triplet(x,y,testsize=0.3,ap_pairs=10,an_pairs=10):\n",
    "    data_xy = tuple([x,y])\n",
    "\n",
    "    trainsize = 1-testsize\n",
    "\n",
    "    triplet_train_pairs = []\n",
    "    triplet_test_pairs = []\n",
    "    for data_class in sorted(set(data_xy[1])):\n",
    "\n",
    "        same_class_idx = np.where((data_xy[1] == data_class))[0]\n",
    "        diff_class_idx = np.where(data_xy[1] != data_class)[0]\n",
    "        A_P_pairs = random.sample(list(permutations(same_class_idx,2)),k=ap_pairs) #Generating Anchor-Positive pairs\n",
    "        Neg_idx = random.sample(list(diff_class_idx),k=an_pairs)\n",
    "        \n",
    "\n",
    "        #train\n",
    "        A_P_len = len(A_P_pairs)\n",
    "        Neg_len = len(Neg_idx)\n",
    "        for ap in A_P_pairs[:int(A_P_len*trainsize)]:\n",
    "            Anchor = data_xy[0][ap[0]]\n",
    "            Positive = data_xy[0][ap[1]]\n",
    "            for n in Neg_idx:\n",
    "                Negative = data_xy[0][n]\n",
    "                triplet_train_pairs.append([Anchor,Positive,Negative])               \n",
    "        #test\n",
    "        for ap in A_P_pairs[int(A_P_len*trainsize):]:\n",
    "            Anchor = data_xy[0][ap[0]]\n",
    "            Positive = data_xy[0][ap[1]]\n",
    "            for n in Neg_idx:\n",
    "                Negative = data_xy[0][n]\n",
    "                triplet_test_pairs.append([Anchor,Positive,Negative])    \n",
    "                \n",
    "    return np.array(triplet_train_pairs), np.array(triplet_test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(y)\n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3060, 105, 105, 3)\n"
     ]
    }
   ],
   "source": [
    "save('X.npy', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = generate_triplet(X[0:1500], y[0:1500], ap_pairs=50, an_pairs=50,testsize=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import save\n",
    "save('X_train.npy', X_train)\n",
    "save('X_test.npy', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.4):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss function\n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor data\n",
    "            positive -- the encodings for the positive data (similar to anchor)\n",
    "            negative -- the encodings for the negative data (different from anchor)\n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    print('y_pred.shape = ',y_pred)\n",
    "    \n",
    "    total_lenght = y_pred.shape.as_list()[-1]\n",
    "#     print('total_lenght=',  total_lenght)\n",
    "#     total_lenght =12\n",
    "    \n",
    "    anchor = y_pred[:,0:int(total_lenght*1/3)]\n",
    "    positive = y_pred[:,int(total_lenght*1/3):int(total_lenght*2/3)]\n",
    "    negative = y_pred[:,int(total_lenght*2/3):int(total_lenght*3/3)]\n",
    "\n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = K.sum(K.square(anchor-positive),axis=1)\n",
    "\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n",
    "\n",
    "    # compute loss\n",
    "    basic_loss = pos_dist-neg_dist+alpha\n",
    "    loss = K.maximum(basic_loss,0.0)\n",
    " \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_shape):\n",
    "    \"\"\"\n",
    "    Base network to be shared.\n",
    "    \"\"\"\n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optim = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.shape =  Tensor(\"merged_layer/concat:0\", shape=(None, 12288), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "anchor_input = Input((105,105,3, ), name='anchor_input')\n",
    "positive_input = Input((105,105,3, ), name='positive_input')\n",
    "negative_input = Input((105,105,3, ), name='negative_input')\n",
    "\n",
    "# Shared embedding layer for positive and negative items\n",
    "Shared_DNN = create_base_network([105,105,3,])\n",
    "\n",
    "\n",
    "encoded_anchor = Shared_DNN(anchor_input)\n",
    "encoded_positive = Shared_DNN(positive_input)\n",
    "encoded_negative = Shared_DNN(negative_input)\n",
    "\n",
    "\n",
    "merged_vector = concatenate([encoded_anchor, encoded_positive, encoded_negative], axis=-1, name='merged_layer')\n",
    "\n",
    "model = Model(inputs=[anchor_input,positive_input, negative_input], outputs=merged_vector)\n",
    "model.compile(loss=triplet_loss, optimizer=adam_optim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_input (InputLayer)     (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_input (InputLayer)     (None, 105, 105, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 4096)         38960448    anchor_input[0][0]               \n",
      "                                                                 positive_input[0][0]             \n",
      "                                                                 negative_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "merged_layer (Concatenate)      (None, 12288)        0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "                                                                 sequential_1[3][0]               \n",
      "==================================================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 25000 samples\n",
      "Epoch 1/500\n"
     ]
    }
   ],
   "source": [
    "Anchor = X_train[:,0,:].reshape(-1,105,105,3)\n",
    "Positive = X_train[:,1,:].reshape(-1,105,105,3)\n",
    "Negative = X_train[:,2,:].reshape(-1,105,105,3)\n",
    "Anchor_test = X_test[:,0,:].reshape(-1,105,105,3)\n",
    "Positive_test = X_test[:,1,:].reshape(-1,105,105,3)\n",
    "Negative_test = X_test[:,2,:].reshape(-1,105,105,3)\n",
    "\n",
    "Y_dummy = np.empty((Anchor.shape[0],300))\n",
    "Y_dummy2 = np.empty((Anchor_test.shape[0],1))\n",
    "\n",
    "model.fit([Anchor,Positive,Negative],y=Y_dummy,validation_data=([Anchor_test,Positive_test,Negative_test],Y_dummy2), batch_size=512, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model.h5\")\n",
    "    print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-db7eb82ac784>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
